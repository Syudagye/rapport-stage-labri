@misc{evalcodecompquality,
    title = {Evaluating the Code Quality of AI-Assisted Code Generation Tools:
             An Empirical Study on GitHub Copilot, Amazon CodeWhisperer, and
             ChatGPT},
    author = {Burak Yetiştiren and Işık Özsoy and Miray Ayerdem and Eray Tüzün},
    year = {2023},
    eprint = {2304.10778},
    archivePrefix = {arXiv},
    primaryClass = {cs.SE},
    url = {https://arxiv.org/abs/2304.10778},
}

@misc{llm-online-offline,
    title = {Language Models for Code Completion: A Practical Evaluation},
    author = {Maliheh Izadi and Jonathan Katzy and Tim van Dam and Marc Otten
              and Razvan Mihai Popescu and Arie van Deursen},
    year = {2024},
    eprint = {2402.16197},
    archivePrefix = {arXiv},
    primaryClass = {cs.SE},
    url = {https://arxiv.org/abs/2402.16197},
}

@article{grouded,
    author = {Barke, Shraddha and James, Michael B. and Polikarpova, Nadia},
    title = {Grounded Copilot: How Programmers Interact with Code-Generating
             Models},
    year = {2023},
    issue_date = {April 2023},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    volume = {7},
    number = {OOPSLA1},
    url = {https://doi.org/10.1145/3586030},
    doi = {10.1145/3586030},
    abstract = {Powered by recent advances in code-generating models, AI
                assistants like Github Copilot promise to change the face of
                programming forever. But what is this new face of programming? We
                present the first grounded theory analysis of how programmers
                interact with Copilot, based on observing 20 participants—with a
                range of prior experience using the assistant—as they solve
                diverse programming tasks across four languages. Our main finding
                is that interactions with programming assistants are bimodal: in
                acceleration mode, the programmer knows what to do next and uses
                Copilot to get there faster; in exploration mode, the programmer
                is unsure how to proceed and uses Copilot to explore their
                options. Based on our theory, we provide recommendations for
                improving the usability of future AI programming assistants.},
    journal = {Proc. ACM Program. Lang.},
    month = {apr},
    articleno = {78},
    numpages = {27},
    keywords = {AI Assistants, Grounded Theory, Program Synthesis},
}

@inproceedings{codeaid,
    author = {Kazemitabaar, Majeed and Ye, Runlong and Wang, Xiaoning and Henley
              , Austin Zachary and Denny, Paul and Craig, Michelle and Grossman,
              Tovi},
    title = {CodeAid: Evaluating a Classroom Deployment of an LLM-based
             Programming Assistant that Balances Student and Educator Needs},
    year = {2024},
    isbn = {9798400703300},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    url = {https://doi.org/10.1145/3613904.3642773},
    doi = {10.1145/3613904.3642773},
    abstract = {Timely, personalized feedback is essential for students learning
                programming. LLM-powered tools like ChatGPT offer instant support
                , but reveal direct answers with code, which may hinder deep
                conceptual engagement. We developed CodeAid, an LLM-powered
                programming assistant delivering helpful, technically correct
                responses, without revealing code solutions. CodeAid answers
                conceptual questions, generates pseudo-code with line-by-line
                explanations, and annotates student’s incorrect code with fix
                suggestions. We deployed CodeAid in a programming class of 700
                students for a 12-week semester. A thematic analysis of 8,000
                usages of CodeAid was performed, further enriched by weekly
                surveys, and 22 student interviews. We then interviewed eight
                programming educators to gain further insights. Our findings
                reveal four design considerations for future educational AI
                assistants: D1) exploiting AI’s unique benefits; D2) simplifying
                query formulation while promoting cognitive engagement; D3)
                avoiding direct responses while encouraging motivated learning;
                and D4) maintaining transparency and control for students to
                asses and steer AI responses.},
    booktitle = {Proceedings of the CHI Conference on Human Factors in Computing
                 Systems},
    articleno = {650},
    numpages = {20},
    keywords = {AI assistants, AI tutoring, class deployment, design guidelines,
                educational technology, generative AI, intelligent tutoring
                systems, large language models, programming education},
    location = {Honolulu, HI, USA},
    series = {CHI '24},
}

@inproceedings{productivity-assess,
    author = {Ziegler, Albert and Kalliamvakou, Eirini and Li, X. Alice and Rice
              , Andrew and Rifkin, Devon and Simister, Shawn and Sittampalam,
              Ganesh and Aftandilian, Edward},
    title = {Productivity assessment of neural code completion},
    year = {2022},
    isbn = {9781450392730},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    url = {https://doi.org/10.1145/3520312.3534864},
    doi = {10.1145/3520312.3534864},
    abstract = {Neural code synthesis has reached a point where snippet
                generation is accurate enough to be considered for integration
                into human software development workflows. Commercial products
                aim to increase programmers’ productivity, without being able to
                measure it directly. In this case study, we asked users of GitHub
                Copilot about its impact on their productivity, and sought to
                find a reflection of their perception in directly measurable user
                data. We find that the rate with which shown suggestions are
                accepted, rather than more specific metrics regarding the
                persistence of completions in the code over time, drives
                developers’ perception of productivity.},
    booktitle = {Proceedings of the 6th ACM SIGPLAN International Symposium on
                 Machine Programming},
    pages = {21–29},
    numpages = {9},
    keywords = {code completion, code synthesis, neural networks, productivity},
    location = {San Diego, CA, USA},
    series = {MAPS 2022},
}
